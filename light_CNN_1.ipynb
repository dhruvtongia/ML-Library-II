{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "from torch.nn import Parameter\n",
    "from torchvision import datasets,transforms\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(X,W=None):\n",
    "    \"\"\"\n",
    "    computes the linear_activation of the scalar or vector X ,W and bias b\n",
    "    \n",
    "    arguments:\n",
    "    X -- data from previous layer of size-> (size of previous layer, number of examples)\n",
    "     \n",
    "    W -- parameters or weights matrix , numpy array of shape -> (size of current layer, size of previous layer)\n",
    "    \n",
    "    \n",
    "    Z --- linear_activation of W,X and b\n",
    "    \n",
    "    \"\"\"\n",
    "    Z=np.dot(W,X)\n",
    "    \n",
    "    return Z\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self,w):\n",
    "        super(Linear,self).__init__()\n",
    "        self.in_features=in_features\n",
    "        \n",
    "        if W==none:\n",
    "            self.w=Parameter(torch.tensor(1,1))\n",
    "        else:\n",
    "            self.w=Parameter(torch.tensor(W))\n",
    "        \n",
    "        \n",
    "            \n",
    "        self.w.requiresGrad=True\n",
    "        self.b.requiresGrad=True\n",
    "    def forward(self,X,W=None):\n",
    "        return linear_activation(Z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(Z):\n",
    "    \"\"\" \n",
    "    computes the sigmoid_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    s --- sigmoid of Z\n",
    "    \n",
    "    \"\"\"\n",
    "    s=torch.sigmoid(Z)\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "class Sigmoid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,Z):\n",
    "        return sigmoid_activation(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(Z):\n",
    "    \"\"\" \n",
    "    computes the relu_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    relu --- relu of Z\n",
    "    \"\"\"\n",
    "    \n",
    "    relu=torch.max(Z,torch.tensor(0.0))\n",
    "    \n",
    "    return relu\n",
    "\n",
    "class Relu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,Z):\n",
    "        return relu_activation(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the leaky_relu_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    leaky_relu --- leaky_relu of Z\n",
    "    \"\"\"\n",
    "    \n",
    "    leaky_relu= torch.max(Z*0.01,Z)\n",
    "    \n",
    "    return leaky_relu\n",
    "\n",
    "class Leaky_Relu(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,Z):\n",
    "        return leaky_relu_activation(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the softplus_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    sofplus --- softplus of Z\n",
    "    \"\"\"\n",
    "    softplus=torch.log(1+torch.exp(Z))\n",
    "    \n",
    "    return softplus\n",
    "\n",
    "class Softplus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,Z):\n",
    "        return softplus_activation(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the tanh_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    tanh --- tanh of Z\n",
    "    \"\"\"\n",
    "    tanh=torch.tanh(Z)\n",
    "    \n",
    "    return tanh\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,Z):\n",
    "        return tanh_activation(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the swish_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    swish --- swish of Z\n",
    "    \"\"\"\n",
    "    return Z*torch.sigmoid(Z)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,Z):\n",
    "        return swish_activation(Z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the mish_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    mish --- mish of Z\n",
    "    \"\"\"\n",
    "    mish=Z*(torch.tanh(torch.log(1+torch.exp(Z))))\n",
    "    \n",
    "    return mish\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,Z):\n",
    "        return mish_activation(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=datasets.MNIST(\"\",train=True,download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test=datasets.MNIST(\"\",train=False,download=True,transform=transforms.Compose([transforms.ToTensor()]))\n",
    "testset=torch.utils.data.DataLoader(test,batch_size=256, shuffle=True)\n",
    "train_size = int(0.9 * len(train))\n",
    "val_size = len(train) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train, [train_size, val_size])\n",
    "train_dataset=torch.utils.data.DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "val_dataset=torch.utils.data.DataLoader(val_dataset,batch_size=256,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net(\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self,activation):\n",
    "        super().__init__()\n",
    "        #self.activation=activation()\n",
    "        self.fc1=nn.Linear(256,256)\n",
    "        self.fc2=nn.Linear(256,128)\n",
    "        self.fc3=nn.Linear(128,10)\n",
    "        self.conv1=nn.Conv2d(1,32,kernel_size=2)\n",
    "        self.conv2=nn.Conv2d(32,64,kernel_size=2)\n",
    "        self.conv3=nn.Conv2d(64,128,kernel_size=2)\n",
    "        self.conv4=nn.Conv2d(128,256,kernel_size=2)\n",
    "\n",
    "    def forward(self,x,activation):\n",
    "        \n",
    "        #print(x.size())\n",
    "        \n",
    "        x=F.max_pool2d(activation(self.conv1(x)),(2,2))\n",
    "        #print(x.size())\n",
    "        \n",
    "        x=F.max_pool2d(activation(self.conv2(x)),(2,2))\n",
    "        #print(x.size())\n",
    "        x=F.max_pool2d(activation(self.conv3(x)),(2,2))\n",
    "        \n",
    "        x=F.max_pool2d(activation(self.conv4(x)),(1,1))\n",
    "        #print(x.size())\n",
    "        x=x.view(x.size(0),-1)\n",
    "        #print(x.size())\n",
    "        x=activation(self.fc1(x))\n",
    "        #print(x.size())\n",
    "        x=activation(self.fc2(x))\n",
    "        #print(x.size())\n",
    "        x=self.fc3(x)\n",
    "        #print(x.size())\n",
    "        return x       \n",
    "        \n",
    "Net=net(activation)\n",
    "print(Net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =optim.Adam(Net.parameters())\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " sigmoid \n",
      "\n",
      "epoch: {0} loss: {2.3052448932593466} time: {102.93439817428589} \n",
      "\n",
      "epoch: {1} loss: {1.7477094488686296} time: {112.98895573616028} \n",
      "\n",
      "epoch: {2} loss: {1.0261735142124773} time: {115.841712474823} \n",
      "\n",
      "epoch: {3} loss: {0.706811302928563} time: {125.36805748939514} \n",
      "\n",
      "epoch: {4} loss: {0.5115796300754728} time: {117.2865662574768} \n",
      "\n",
      "validation loss: {0.4167787916958332} Accuracy:  1.0 \n",
      "\n",
      "\n",
      " relu \n",
      "\n",
      "epoch: {0} loss: {155.50649305598995} time: {117.26580715179443} \n",
      "\n",
      "epoch: {1} loss: {0.8334494357425455} time: {117.08595013618469} \n",
      "\n",
      "epoch: {2} loss: {0.6578138402852967} time: {115.02333950996399} \n",
      "\n",
      "epoch: {3} loss: {0.5400283083814015} time: {115.67744278907776} \n",
      "\n",
      "epoch: {4} loss: {0.4540926958147383} time: {118.08559203147888} \n",
      "\n",
      "validation loss: {0.39596694707870483} Accuracy:  1.0 \n",
      "\n",
      "\n",
      " leaky_relu \n",
      "\n",
      "epoch: {0} loss: {0.42398359411135667} time: {133.88179874420166} \n",
      "\n",
      "epoch: {1} loss: {0.3286853389576148} time: {131.48746967315674} \n",
      "\n",
      "epoch: {2} loss: {0.2841316439670409} time: {131.2750208377838} \n",
      "\n",
      "epoch: {3} loss: {0.25397333425085694} time: {131.80372714996338} \n",
      "\n",
      "epoch: {4} loss: {0.22894790108311233} time: {130.5376899242401} \n",
      "\n",
      "validation loss: {0.22535548421243826} Accuracy:  1.0 \n",
      "\n",
      "\n",
      " softplus \n",
      "\n",
      "epoch: {0} loss: {nan} time: {118.28272414207458} \n",
      "\n",
      "epoch: {1} loss: {nan} time: {118.81700778007507} \n",
      "\n",
      "epoch: {2} loss: {nan} time: {173.99339175224304} \n",
      "\n",
      "epoch: {3} loss: {nan} time: {127.87038707733154} \n",
      "\n",
      "epoch: {4} loss: {nan} time: {118.87430453300476} \n",
      "\n",
      "validation loss: {nan} Accuracy:  1.0 \n",
      "\n",
      "\n",
      " tanh \n",
      "\n",
      "epoch: {0} loss: {nan} time: {107.7273199558258} \n",
      "\n",
      "epoch: {1} loss: {nan} time: {107.88456106185913} \n",
      "\n",
      "epoch: {2} loss: {nan} time: {108.69887185096741} \n",
      "\n",
      "epoch: {3} loss: {nan} time: {106.4850206375122} \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2ad51bc87d87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[0mNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mloss_train\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "activation_functions={ 'sigmoid':Sigmoid() , 'relu':Relu() ,'leaky_relu':Leaky_Relu() , 'softplus':Softplus() , 'tanh':Tanh(),\n",
    "                     'swish':Swish() , 'mish':Mish()}\n",
    "\n",
    "for activation in activation_functions:\n",
    "    print('\\n',activation,'\\n')\n",
    "    #training\n",
    "    epochs=5\n",
    "    for epoch in range(epochs):\n",
    "        start=time.time()\n",
    "        loss_final=0\n",
    "        loss_train=0\n",
    "        for data_train in train_dataset:\n",
    "        \n",
    "            x,y=data_train\n",
    "            #print(y)\n",
    "        \n",
    "            output=Net(x,activation_functions[activation])\n",
    "            #print(output)\n",
    "            Net.zero_grad()\n",
    "            loss=loss_function(output,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train+=loss.item()\n",
    "        end=time.time()\n",
    "        loss_final=loss_train/len(train_dataset)\n",
    "        \n",
    "        print(\"epoch:\", {epoch} , \"loss:\" ,{loss_final},'time:' ,{end-start} ,'\\n')\n",
    "        \n",
    "        #validation\n",
    "        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_val_loss=0\n",
    "    with torch.no_grad():\n",
    "         for data_val in val_dataset:\n",
    "            image,label=data_val\n",
    "            real_class = torch.argmax(image)\n",
    "            net_out = Net(image,activation_functions[activation])  \n",
    "            val_loss=loss_function(net_out,label)\n",
    "            total_val_loss+=val_loss.item()\n",
    "            for i,p in enumerate(net_out):\n",
    "                if label[i]==torch.max(p,0)[1]:  # torch.max(p,0) calculates the maximum value along with its indices and returms a 2d tensor and by using [1] we take its indices\n",
    "                    correct+=1\n",
    "                    total+=1\n",
    "    print(\"validation loss:\",{total_val_loss/len(val_dataset)},\"Accuracy: \", round(correct/total, 3),'\\n')\n",
    "    \n",
    "    #Accuracy is in ratio and not percentage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
