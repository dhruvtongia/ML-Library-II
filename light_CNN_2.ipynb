{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "from torch.nn import Parameter\n",
    "from torchvision import datasets,transforms\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the softplus_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    sofplus --- softplus of Z\n",
    "    \"\"\"\n",
    "    softplus=torch.log(1+torch.exp(Z))\n",
    "    \n",
    "    return softplus\n",
    "\n",
    "class Softplus(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,Z):\n",
    "        return softplus_activation(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the tanh_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    tanh --- tanh of Z\n",
    "    \"\"\"\n",
    "    tanh=torch.tanh(Z)\n",
    "    \n",
    "    return tanh\n",
    "\n",
    "class Tanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,Z):\n",
    "        return tanh_activation(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the swish_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    swish --- swish of Z\n",
    "    \"\"\"\n",
    "    return Z*torch.sigmoid(Z)\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,Z):\n",
    "        return swish_activation(Z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the mish_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    mish --- mish of Z\n",
    "    \"\"\"\n",
    "    mish=Z*(torch.tanh(torch.log(1+torch.exp(Z))))\n",
    "    \n",
    "    return mish\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,Z):\n",
    "        return mish_activation(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=datasets.MNIST(\"\",train=True,download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test=datasets.MNIST(\"\",train=False,download=True,transform=transforms.Compose([transforms.ToTensor()]))\n",
    "testset=torch.utils.data.DataLoader(test,batch_size=256, shuffle=True)\n",
    "train_size = int(0.9 * len(train))\n",
    "val_size = len(train) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train, [train_size, val_size])\n",
    "train_dataset=torch.utils.data.DataLoader(train_dataset,batch_size=256,shuffle=True)\n",
    "val_dataset=torch.utils.data.DataLoader(val_dataset,batch_size=256,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net(\n",
      "  (fc1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(2, 2), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self,activation):\n",
    "        super().__init__()\n",
    "        #self.activation=activation()\n",
    "        self.fc1=nn.Linear(256,256)\n",
    "        self.fc2=nn.Linear(256,128)\n",
    "        self.fc3=nn.Linear(128,10)\n",
    "        self.conv1=nn.Conv2d(1,32,kernel_size=2)\n",
    "        self.conv2=nn.Conv2d(32,64,kernel_size=2)\n",
    "        self.conv3=nn.Conv2d(64,128,kernel_size=2)\n",
    "        self.conv4=nn.Conv2d(128,256,kernel_size=2)\n",
    "\n",
    "    def forward(self,x,activation):\n",
    "        \n",
    "        #print(x.size())\n",
    "        \n",
    "        x=F.max_pool2d(activation(self.conv1(x)),(2,2))\n",
    "        #print(x.size())\n",
    "        \n",
    "        x=F.max_pool2d(activation(self.conv2(x)),(2,2))\n",
    "        #print(x.size())\n",
    "        x=F.max_pool2d(activation(self.conv3(x)),(2,2))\n",
    "        \n",
    "        x=F.max_pool2d(activation(self.conv4(x)),(1,1))\n",
    "        #print(x.size())\n",
    "        x=x.view(x.size(0),-1)\n",
    "        #print(x.size())\n",
    "        x=activation(self.fc1(x))\n",
    "        #print(x.size())\n",
    "        x=activation(self.fc2(x))\n",
    "        #print(x.size())\n",
    "        x=self.fc3(x)\n",
    "        #print(x.size())\n",
    "        return x       \n",
    "        \n",
    "Net=net(activation)\n",
    "print(Net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =optim.Adam(Net.parameters())\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " softplus \n",
      "\n",
      "epoch: {0} loss: {1.6385494544042796} time: {171.85923409461975} \n",
      "\n",
      "epoch: {1} loss: {0.3513472583056626} time: {173.3243043422699} \n",
      "\n",
      "epoch: {2} loss: {0.19736817927580874} time: {175.6757025718689} \n",
      "\n",
      "epoch: {3} loss: {0.13560946096811816} time: {175.922602891922} \n",
      "\n",
      "epoch: {4} loss: {0.09753877623668779} time: {175.08676838874817} \n",
      "\n",
      "validation loss: {0.11911824407676856} Accuracy:  1.0 \n",
      "\n",
      "\n",
      " tanh \n",
      "\n",
      "epoch: {0} loss: {0.2126905798770805} time: {159.35404706001282} \n",
      "\n",
      "epoch: {1} loss: {0.07592511978618341} time: {158.2499134540558} \n",
      "\n",
      "epoch: {2} loss: {0.054935668325890294} time: {157.91201257705688} \n",
      "\n",
      "epoch: {3} loss: {0.043647996714930117} time: {158.48081755638123} \n",
      "\n",
      "epoch: {4} loss: {0.03278982912199086} time: {160.1762056350708} \n",
      "\n",
      "validation loss: {0.07177762365123878} Accuracy:  1.0 \n",
      "\n",
      "\n",
      " swish \n",
      "\n",
      "epoch: {0} loss: {0.1456856356218669} time: {208.79032611846924} \n",
      "\n",
      "epoch: {1} loss: {0.045578881678882084} time: {206.36493635177612} \n",
      "\n",
      "epoch: {2} loss: {0.03239683791447731} time: {207.0101990699768} \n",
      "\n",
      "epoch: {3} loss: {0.026934428956531763} time: {207.58451199531555} \n",
      "\n",
      "epoch: {4} loss: {0.02154371879059133} time: {206.29373812675476} \n",
      "\n",
      "validation loss: {0.08114620445606609} Accuracy:  1.0 \n",
      "\n",
      "\n",
      " mish \n",
      "\n",
      "epoch: {0} loss: {0.021446424437095345} time: {207.2110857963562} \n",
      "\n",
      "epoch: {1} loss: {0.015584685262877459} time: {206.45695686340332} \n",
      "\n",
      "epoch: {2} loss: {0.014827218327408666} time: {203.54453229904175} \n",
      "\n",
      "epoch: {3} loss: {0.009722455951326717} time: {210.2647078037262} \n",
      "\n",
      "epoch: {4} loss: {0.009898441322716129} time: {209.22905230522156} \n",
      "\n",
      "validation loss: {0.07630515187580993} Accuracy:  1.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activation_functions={ 'softplus':Softplus() , 'tanh':Tanh(),\n",
    "                     'swish':Swish() , 'mish':Mish()}\n",
    "\n",
    "for activation in activation_functions:\n",
    "    print('\\n',activation,'\\n')\n",
    "    #training\n",
    "    epochs=5\n",
    "    for epoch in range(epochs):\n",
    "        start=time.time()\n",
    "        loss_final=0\n",
    "        loss_train=0\n",
    "        for data_train in train_dataset:\n",
    "        \n",
    "            x,y=data_train\n",
    "            #print(y)\n",
    "        \n",
    "            output=Net(x,activation_functions[activation])\n",
    "            #print(output)\n",
    "            Net.zero_grad()\n",
    "            loss=loss_function(output,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train+=loss.item()\n",
    "        end=time.time()\n",
    "        loss_final=loss_train/len(train_dataset)\n",
    "        \n",
    "        print(\"epoch:\", {epoch} , \"loss:\" ,{loss_final},'time:' ,{end-start} ,'\\n')\n",
    "        \n",
    "        #validation\n",
    "        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_val_loss=0\n",
    "    with torch.no_grad():\n",
    "         for data_val in val_dataset:\n",
    "            image,label=data_val\n",
    "            real_class = torch.argmax(image)\n",
    "            net_out = Net(image,activation_functions[activation])  \n",
    "            val_loss=loss_function(net_out,label)\n",
    "            total_val_loss+=val_loss.item()\n",
    "            for i,p in enumerate(net_out):\n",
    "                if label[i]==torch.max(p,0)[1]:  # torch.max(p,0) calculates the maximum value along with its indices and returms a 2d tensor and by using [1] we take its indices\n",
    "                    correct+=1\n",
    "                    total+=1\n",
    "    print(\"validation loss:\",{total_val_loss/len(val_dataset)},\"Accuracy: \", round(correct/total, 3),'\\n')\n",
    "    \n",
    "    #Accuracy is in ratio and not percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
