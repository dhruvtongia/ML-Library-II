{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation(W,X,b):\n",
    "    \"\"\"\n",
    "    computes the linear_activation of the scalar or vector X ,W and bias b\n",
    "    \n",
    "    arguments:\n",
    "    X -- data from previous layer of size-> (size of previous layer, number of examples)\n",
    "     \n",
    "    W -- parameters or weights matrix , numpy array of shape -> (size of current layer, size of previous layer)\n",
    "    \n",
    "    b -- bias vector, numpy array of shape -> (size of the current layer, 1)\n",
    "    returns:\n",
    "    \n",
    "    Z --- linear_activation of W,X and b\n",
    "    \n",
    "    \"\"\"\n",
    "    Z=np.dot(W,X)+b\n",
    "    \n",
    "    return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(Z):\n",
    "    \"\"\" \n",
    "    computes the sigmoid_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    s --- sigmoid of Z\n",
    "    \n",
    "    \"\"\"\n",
    "    s=1/(1+np.exp(-Z))\n",
    "    \n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_activation(Z):\n",
    "    \"\"\" \n",
    "    computes the relu_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    relu --- relu of Z\n",
    "    \"\"\"\n",
    "    relu=np.maximum(0,Z)\n",
    "    \n",
    "    return relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the leaky_relu_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    leaky_relu --- leaky_relu of Z\n",
    "    \"\"\"\n",
    "    \n",
    "    leaky_relu= np.maximum(z*0.01,Z)\n",
    "    \n",
    "    return leaky_relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the softplus_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    sofplus --- softplus of Z\n",
    "    \"\"\"\n",
    "    softplus=np.log(1+np.exp(Z))\n",
    "    \n",
    "    return softplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the tanh_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    tanh --- tanh of Z\n",
    "    \"\"\"\n",
    "    tanh=np.tanh(Z)\n",
    "    \n",
    "    return tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the swish_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    swish --- swish of Z\n",
    "    \"\"\"\n",
    "    swish=Z*(1/(1+np.exp(-Z)))\n",
    "    \n",
    "    return swish\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish_activation(Z):\n",
    "    \"\"\"\n",
    "    computes the mish_activation of the scalar or vector Z\n",
    "    \n",
    "    arguments:\n",
    "    Z --- a scalar or vector of any size\n",
    "    \n",
    "    returns:\n",
    "    mish --- mish of Z\n",
    "    \"\"\"\n",
    "    mish=z*(np.tanh(np.log(1+np.exp(Z))))\n",
    "    \n",
    "    return mish"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
